"""
LLM service for dynamic query analysis and improvement.
Implements workflow: Initial retrieval → LLM analysis → Filter extraction → Question generation
"""

import json
import logging
from typing import List, Dict, Any, Optional, Tuple
from pydantic import BaseModel
import ollama
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.prompt import Prompt, Confirm

logger = logging.getLogger(__name__)


class DynamicQuestion(BaseModel):
    """A targeted question generated by LLM to gather missing information."""
    question: str
    # filter_type: str  # behavior, breed, age, study_type, methodology, sample_size
    options: List[str] = []
    priority: int = 1  # 1 = high, 2 = medium, 3 = low
    reasoning: str = ""  # Why this question is important


class LLMQueryAnalyzer:
    """
    Main class implementing the LLM-based dynamic RAG workflow.
    LLM-powered query analyzer that dynamically improves queries based on retrieved context.
    
    Workflow:
    1. Initial retrieval from ChromaDB based on user query
    2. LLM analyzes retrieved text + user query to extract relevant questions for the user.  
    3. LLM formulates targeted questions to gather missing information.
    4. Questions asked to user before final response
    """
    def __init__(self, vector_db, model_name: str = "llama3.2:3b"):
        """Initialize with specified Ollama model."""
        self.model_name = model_name
        self.vector_db = vector_db
        self.client = ollama.Client(host='http://localhost:11434')
        try:
            self.client.list()
            logger.info("Connected successfully!")
        except Exception as e:
            print(f"Can't connect to Ollama: {e}")
        # Verify model availability
        try:
            self.client.show(model_name)
            logger.info(f"LLM model {model_name} is available")
        except Exception as e:
            logger.warning(f"Model {model_name} not available, will try to pull: {e}")
            try:
                self.client.pull(model_name)
                logger.info(f"Successfully pulled model {model_name}")
            except Exception as pull_error:
                logger.error(f"Failed to pull model {model_name}: {pull_error}")
                raise
    
    def generate_targeted_questions(self, user_query: str, retrieved_documents: List[Dict[str, Any]]) -> List[Dict[str, str]]:
        """
        Analyze user query along with retrieved documents to extract relevant filtering questions.
        
        Args:
            user_query: The original user query
            retrieved_documents: List of documents retrieved from initial search
            
        Returns:
            List of prioritized questions to ask the user
        """
        doc_context = self._prepare_document_context(retrieved_documents)
        analysis_prompt = self._create_question_prompt(user_query, doc_context)
        try:
            response = self.client.chat(
                model=self.model_name,
                messages=[{"role": "user", "content": analysis_prompt}]
            )
            
            # Add debugging to see what the LLM is actually returning
            raw_response = response['message']['content']
            logger.info(f"Raw LLM response: {raw_response}")
            
            response_text = self._parse_response(raw_response)
            logger.info(f"Parsed response text: {response_text}")
              # Try to parse JSON
            try:
                data = json.loads(response_text)
            except json.JSONDecodeError:
                # Try to fix common JSON issues
                logger.warning("Initial JSON parse failed, attempting to fix common issues")
                
                # Remove any trailing text after the JSON array
                if response_text.strip().endswith(']'):
                    end_bracket = response_text.rfind(']')
                    response_text = response_text[:end_bracket + 1]
                
                # Try again
                try:
                    data = json.loads(response_text)
                except json.JSONDecodeError as e:
                    logger.error(f"JSON parsing failed even after cleanup: {e}")
                    logger.error(f"Problematic text: {repr(response_text)}")
                    return []
            
            logger.info(f"JSON parsed successfully, found {len(data)} items")

            questions = []
            for item in data:
                questions.append(DynamicQuestion(
                    question=item.get('question', ''),
                    options=item.get('options', []),
                    priority=int(item.get('priority', 3)),
                    reasoning=item.get('reasoning', '')
                ))
                
            return sorted(questions, key=lambda x: x.priority)
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error: {e}")
            logger.error(f"Response text that failed to parse: {response_text}")
            return []
        except Exception as e:
            logger.error(f"Error generating questions: {e}")
            return []
        

    def _parse_response(self, response_text: str) -> str:
        """Parse LLM response for filter extraction."""
        try:
            response_text = response_text.strip()
            
            # Remove reasoning process if present
            if '<think>' in response_text and '</think>' in response_text:
                start_idx = response_text.find('<think>')
                end_idx = response_text.find('</think>') + len('</think>')
                response_text = response_text[:start_idx] + response_text[end_idx:]
                response_text = response_text.strip()
            
            # Extract JSON from code blocks
            if '```json' in response_text:
                start_idx = response_text.find('```json') + 7
                end_idx = response_text.find('```', start_idx)
                if end_idx != -1:
                    return response_text[start_idx:end_idx].strip()
            elif '```' in response_text:
                start_idx = response_text.find('```') + 3
                end_idx = response_text.find('```', start_idx)
                if end_idx != -1:
                    return response_text[start_idx:end_idx].strip()
            
            # If no code blocks, extract JSON array
            if '[' in response_text:
                start_idx = response_text.find('[')
                bracket_count = 0
                end_idx = -1
                for i in range(start_idx, len(response_text)):
                    if response_text[i] == '[':
                        bracket_count += 1
                    elif response_text[i] == ']':
                        bracket_count -= 1
                        if bracket_count == 0:
                            end_idx = i + 1
                            break
                
                if end_idx != -1:
                    return response_text[start_idx:end_idx].strip()
            
            return response_text.strip()
        except Exception as e:
            logger.error(f"Error extracting JSON: {e}")
            return "[]"
        
    def _prepare_document_context(self, documents: List[Dict[str, Any]]) -> str:
            """Prepare a concise context string from retrieved documents."""
            if not documents:
                return "No documents retrieved."
                
            # First we group the documents by their source paper
            papers = {}
            for doc in documents:
                metadata = doc.get('metadata', {})
                paper_id = metadata.get('paper_id', 'unknown')
                if paper_id not in papers:
                    papers[paper_id] = {
                        'title': doc.get('title', 'Unknown Title'),
                        'authors': doc.get('authors', 'Unknown Authors'),
                        'content_chunks': []
                    }
                # papers[paper_id].append(doc)
                papers[paper_id]['content_chunks'].append({'chunk_idx': metadata.get('chunk_index', 10000), 
                                                        'text': doc.get('document', '')})

            # Sort each list by chunk index
            for paper_idx, docs in papers.items():
                docs = sorted(docs['content_chunks'], key=lambda x: x['chunk_idx'])
                final_text = ""
                for i in range(len(docs)):
                    final_text += docs[i]['text']
                    final_text += "[...]" if i != len(docs) or docs[i+1]['chunk_idx'] != docs[i]['chunk_idx'] else ""
                    final_text += "\n"
                papers[paper_idx]['content_chunks'] = final_text.strip()
            
            # concatenate the text from all documents
            context_parts = []
            for paper_id, paper_info in papers.items():
                context_parts.append(f"**Paper: {paper_info['title']}**\n{paper_info['content_chunks']}\n")
            
            return '\n'.join(context_parts).strip()

    def _create_question_prompt(self, user_query: str, doc_context: str) -> str:
        """Create prompt for analyzing query and extracting filters."""
        return f"""# Dog Behavior Research Query Analysis

## Your Role
You are an expert in dog behavior research with deep knowledge of academic literature analysis. Your task is to analyze a user's research question and relevant document excerpts to generate precise clarifying questions.

**IMPORTANT: The questions you generate will be presented directly to the user to help refine their search. Make them user-friendly and easy to understand.**

## Input Data
**User Query:** "{user_query}"

**Retrieved Research Context:**
```
{doc_context}
```

## Task Objective
Generate clarifying questions that will be asked back to the user to improve their search results. These questions should help narrow down their research focus based on what you see in the retrieved documents.

## Question Generation Rules
- Ask ONLY if clarification would significantly narrow search results
- Base questions on patterns you see in the retrieved documents
- Maximum 5 questions, prioritize most impactful
- Use simple, direct language that a researcher would understand
- If query is already specific enough, ask no questions
- Make questions answerable in 1-2 words or a short phrase
- Frame questions as if speaking directly to the user

## Examples of Good Questions:
- "Which specific age group are you focusing on?" (with options: puppy, adult, senior)
- "Are you interested in a particular breed type?" (with options: working dogs, toy breeds, etc.)
- "What research methodology do you prefer?" (with options: experimental, observational, review)

## Required Output Format
Respond with a JSON array where each question is an object with the following structure:
[
    {{
        "question": "Your question text here",
        "options": ["option1", "option2", "option3"],
        "priority": 1,
        "reasoning": "Why this question is important for search accuracy"
    }}
]

If no clarifying questions are needed, return an empty array: []

Generate user-facing questions now:"""


    def improve_query_with_answers(self, original_query: str, questions_answers: Dict[str, str]) -> str:
        """
        Improve the original query by incorporating user answers using LLM.
        
        Args:
            original_query: The original user query
            questions: The questions that were asked
            user_answers: User responses to the questions
            
        Returns:
            Improved query string
        """
        improvement_prompt = self._create_improvement_prompt(original_query, questions_answers)
        
        try:
            response = self.client.chat(
                model=self.model_name,
                messages=[{"role": "user", "content": improvement_prompt}]
            )
                
            return self._parse_response(response['message']['content'])
            
        except Exception as e:
            logger.error(f"Error improving query: {e}")
            # Fallback to simple concatenation
            return original_query



    def _create_improvement_prompt(self, original_query: str, answers: Dict[str, str]) -> str:
        """Create prompt for improving query with user answers."""
        qa_text = "\n".join([f"{q}: {', '.join(a)}" for q, a in answers.items()])
        
        return f"""Improve this research query by incorporating the user's answers to clarifying questions.

Original Query: "{original_query}"

User's Clarifications:
{qa_text}

Create an improved, more specific query that incorporates the user's clarifications while maintaining the original intent. The improved query should be natural and search-friendly.

Respond with only the improved query text, no additional formatting or explanation."""
    
    
    async def process_query(self, user_query: str, top_k: int = 30) -> Dict[str, Any]:
        """
        Process user query through the dynamic RAG workflow.
        
        Args:
            user_query: The user's research question
            top_k: Number of documents to retrieve initially
            
        Returns:
            Dictionary containing analysis results and questions for user
        """
        logger.info(f"Processing query: {user_query}")
        
        # Step 1: Initial retrieval from ChromaDB
        initial_results = self.vector_db.search(user_query, top_k=top_k)
        logger.info(f"Retrieved {len(initial_results)} initial documents")
        
        # Step 2: Generate targeted questions if confidence is low
        # questions = []
        questions = self.generate_targeted_questions(
            user_query, initial_results
        )
        logger.info(f"Generated {len(questions)} clarifying questions")
        
        return {
            "original_query": user_query,
            "initial_results": initial_results,
            "questions": questions,
            "needs_clarification": len(questions) > 0
        }
    
    async def finalize_response(self, analysis_result: Dict[str, Any], 
                              user_answers: Dict[str, str], top_k: int = 30) -> Dict[str, Any]:
        """
        Generate final response after user answers clarifying questions.
        
        Args:
            analysis_result: Result from process_query
            user_answers: User's answers to clarifying questions
            
        Returns:
            Final results with improved query and refined search
        """
        original_query = analysis_result["original_query"]
        questions = analysis_result["questions"]
        
        # Step 4: Improve query with user answers
        improved_query = self.improve_query_with_answers(
            original_query, user_answers
        )
        logger.info(f"Improved query: {improved_query}")
        
        # Step 5: Final search with improved query
        final_results = self.vector_db.search(improved_query, top_k=top_k)
        logger.info(f"Final search returned {len(final_results)} documents")
        
        return {
            "original_query": original_query,
            "improved_query": improved_query,
            "user_answers": user_answers,
            "final_results": final_results,
            "total_documents": len(final_results)
        }
    
class LLMAnswerGenerator:
    """
    User-facing LLM that generates comprehensive answers based on refined search results.
    This is the 'front' agent that actually talks to the user.
    """
    
    def __init__(self, model_name: str = "llama3.2:3b"):
        """Initialize the answer generation LLM."""
        self.model_name = model_name
        self.client = ollama.Client(host='http://localhost:11434')
        
        # Verify model availability
        try:
            self.client.show(model_name)
            logger.info(f"Answer generator LLM model {model_name} is available")
        except Exception as e:
            logger.warning(f"Model {model_name} not available: {e}")
            raise
    
    def generate_comprehensive_answer(self, 
                                    original_query: str, 
                                    improved_query: str, 
                                    retrieved_documents: List[Dict[str, Any]],
                                    user_answers: Dict[str, str] = None) -> str:
        """
        Generate a comprehensive, user-friendly answer based on the refined search results.
        
        Args:
            original_query: The user's original question
            improved_query: Query refined by LLMQueryAnalyzer
            retrieved_documents: Documents retrieved with the improved query
            user_answers: User's answers to clarifying questions (optional)
            
        Returns:
            Comprehensive answer string formatted for the user
        """
        doc_context = self._prepare_document_context(retrieved_documents)
        answer_prompt = self._create_answer_prompt(
            original_query, improved_query, doc_context, user_answers
        )
        
        try:
            response = self.client.chat(
                model=self.model_name,
                messages=[
                    {
                        "role": "system", 
                        "content": "You are a helpful dog behavior expert who provides evidence-based advice to dog owners. Always be empathetic, practical, and cite research when appropriate."
                    },
                    {
                        "role": "user", 
                        "content": answer_prompt
                    }
                ]
            )
            
            answer = self._parse_text_response(response['message']['content'])
            logger.info("Generated comprehensive answer successfully")
            return answer
            
        except Exception as e:
            logger.error(f"Error generating comprehensive answer: {e}")
            return self._generate_fallback_answer(original_query, len(retrieved_documents))
    
   
    def _parse_text_response(self, response_text: str) -> str:
        """Parse LLM response for text answers (not JSON)."""
        try:
            response_text = response_text.strip()
            
            # Remove reasoning process if present (between <think> and </think>)
            if '<think>' in response_text and '</think>' in response_text:
                start_idx = response_text.find('<think>')
                end_idx = response_text.find('</think>') + len('</think>')
                response_text = response_text[:start_idx] + response_text[end_idx:]
                response_text = response_text.strip()
                logger.debug("Removed reasoning process from LLM response")
            
            # Remove code block markers if they contain the answer
            if response_text.startswith('```') and response_text.endswith('```'):
                lines = response_text.split('\n')
                if len(lines) > 2:
                    response_text = '\n'.join(lines[1:-1]).strip()
            
            return response_text.strip()
            
        except Exception as e:
            logger.error(f"Error parsing LLM text response: {e}")
            return "I apologize, but I encountered an error generating the response. Please try again."

    def _prepare_document_context(self, documents: List[Dict[str, Any]]) -> str:
            """Prepare a concise context string from retrieved documents."""
            if not documents:
                return "No documents retrieved."
                
            # First we group the documents by their source paper
            papers = {}
            for doc in documents:
                metadata = doc.get('metadata', {})
                paper_id = metadata.get('paper_id', 'unknown')
                if paper_id not in papers:
                    papers[paper_id] = {
                        'title': doc.get('title', 'Unknown Title'),
                        'authors': doc.get('authors', 'Unknown Authors'),
                        'content_chunks': []
                    }
                # papers[paper_id].append(doc)
                papers[paper_id]['content_chunks'].append({'chunk_idx': metadata.get('chunk_index', 10000), 
                                                        'text': doc.get('document', '')})

            # Sort each list by chunk index
            for paper_idx, docs in papers.items():
                docs = sorted(docs['content_chunks'], key=lambda x: x['chunk_idx'])
                final_text = ""
                for i in range(len(docs)):
                    final_text += docs[i]['text']
                    final_text += "[...]" if i != len(docs) or docs[i+1]['chunk_idx'] != docs[i]['chunk_idx'] else ""
                    final_text += "\n"
                papers[paper_idx]['content_chunks'] = final_text.strip()
            
            # concatenate the text from all documents
            context_parts = []
            for paper_id, paper_info in papers.items():
                context_parts.append(f"**Paper: {paper_info['title']}**\n{paper_info['content_chunks']}\n")
            
            return '\n'.join(context_parts).strip()
    
    def _create_answer_prompt(self, 
                            original_query: str, 
                            improved_query: str, 
                            doc_context: str,
                            user_answers: Dict[str, str] = None) -> str:
        """Create prompt for generating the comprehensive user answer."""
        
        user_context = ""
        if user_answers:
            user_context = f"""

**Additional Context from User:**
{self._format_user_answers(user_answers)}"""
        
        if improved_query:
            improved_query = f'**Refined Focus:** "{improved_query}"{user_context}'

        return f"""# Dog Behavior Expert Response

## Your Mission
You are a caring dog behavior expert helping a dog owner with their question. Provide a comprehensive, evidence-based answer that is both scientifically accurate and practically helpful.

## User's Question
**Original Question:** "{original_query}"
{improved_query}

## Available Research Evidence
{doc_context}

## Response Guidelines

### Structure
1. **Direct Answer**: Start with a clear, direct response to their question
2. **Research Evidence**: Support your answer with findings from the papers
3. **Practical Advice**: Provide actionable steps they can take
4. **Important Considerations**: Include any caveats or when to seek professional help

### Tone & Style
- Be empathetic and understanding (they care about their dog!)
- Use accessible language (avoid excessive jargon)
- Be confident but acknowledge limitations
- Include specific research findings when relevant

### Content Requirements
- Answer the original question directly
- Reference specific studies when making claims
- Provide practical, actionable advice
- Mention when professional help might be needed
- Be honest about research limitations

## Important Notes
- Only make claims supported by the provided research
- If studies contradict each other, acknowledge this
- Don't diagnose medical conditions
- Suggest veterinary consultation for health concerns

Generate your expert response now:"""
    
    def _format_user_answers(self, user_answers: Dict[str, str]) -> str:
        """Format user answers for context."""
        if not user_answers:
            return "No additional context provided."
        
        formatted = []
        for question, answers in user_answers.items():
            if isinstance(answers, list):
                answer_text = ', '.join(answers)
            else:
                answer_text = str(answers)
            formatted.append(f"- {question}: {answer_text}")
        
        return '\n'.join(formatted)
    
    def _generate_fallback_answer(self, query: str, num_docs: int) -> str:
        """Generate a fallback answer when LLM fails."""
        return f"""I apologize, but I encountered an error while generating your answer. 

However, I was able to find {num_docs} relevant research papers related to your question: "{query}"

I recommend consulting with a veterinary behaviorist or certified dog trainer who can provide personalized advice based on your specific situation. You can also try rephrasing your question or being more specific about your dog's behavior, age, breed, or circumstances.

If this is an urgent behavioral issue, please consult with a professional immediately."""


# Update the LLMDynamicRAG class to use the new architecture
class LLMDynamicRAG:
    """
    Orchestrates the complete LLM-based RAG workflow with separate backend and frontend LLMs.
    
    Architecture:
    - LLMQueryAnalyzer: Backend agent for query refinement and question generation
    - LLMAnswerGenerator: Frontend agent for user-facing answer generation
    """
    
    def __init__(self, vector_db, query_analyzer_model: str = "llama3.2:3b", answer_model: str = "llama3.2:3b"):
        """
        Initialize with separate models for query analysis and answer generation.
        
        Args:
            vector_db: Vector database instance
            query_analyzer_model: Model for backend query analysis
            answer_model: Model for frontend answer generation
        """
        self.vector_db = vector_db
        self.query_analyzer = LLMQueryAnalyzer(vector_db, query_analyzer_model)
        self.answer_generator = LLMAnswerGenerator(answer_model)
        self.top_k = 30
        
        logger.info("Initialized LLMDynamicRAG with separate query analyzer and answer generator")
    
    async def process_query(self, user_query: str, top_k: int = 30) -> Dict[str, Any]:
        """
        Process user query through the backend analyzer to generate questions.
        
        Args:
            user_query: The user's research question
            top_k: Number of documents to retrieve initially
            
        Returns:
            Dictionary containing analysis results and questions for user
        """
        return await self.query_analyzer.process_query(user_query, top_k)
    
    async def finalize_response(self, analysis_result: Dict[str, Any], 
                              user_answers: Dict[str, str]) -> Dict[str, Any]:
        """
        Generate final response with improved query and comprehensive answer.
        
        Args:
            analysis_result: Result from process_query (backend analyzer)
            user_answers: User's answers to clarifying questions
            
        Returns:
            Dictionary with improved query, final results, and comprehensive answer
        """
        original_query = analysis_result["original_query"]
        
        # Step 1: Use backend analyzer to improve the query
        improved_query = self.query_analyzer.improve_query_with_answers(original_query, user_answers)
        logger.info(f"Backend analyzer improved query: {improved_query}")
        
        # Step 2: Search with improved query
        final_results = self.vector_db.search(improved_query, top_k=self.top_k)
        logger.info(f"Retrieved {len(final_results)} documents with improved query")
        
        # Step 3: Use frontend generator to create comprehensive answer
        final_answer = self.answer_generator.generate_comprehensive_answer(
            original_query, improved_query, final_results, user_answers
        )
        logger.info("Frontend generator created comprehensive answer")
        
        return {
            "improved_query": improved_query,
            "final_results": final_results,
            "final_answer": final_answer,
            "num_sources": len(final_results)
        }
    
    async def generate_direct_answer(self, user_query: str, top_k: int = 30) -> Dict[str, Any]:
        """
        Generate answer directly without question refinement (for simple/clear queries).
        
        Args:
            user_query: The user's question
            top_k: Number of documents to retrieve
            
        Returns:
            Dictionary with results and direct answer
        """
        # Get documents
        results = self.vector_db.search(user_query, top_k=top_k)
        logger.info(f"Direct search retrieved {len(results)} documents")
        
        # Generate answer
        answer = self.answer_generator.generate_comprehensive_answer(
            original_query=user_query, improved_query=None, retrieved_documents=results, user_answers=None
        )
        
        return {
            "query": user_query,
            "results": results,
            "answer": answer,
            "num_sources": len(results)
        }
    
    def debug_llm_response(self) -> bool:
        """Debug method to test if LLM can return simple JSON."""
        test_prompt = """Return a simple JSON array with one question:
[
    {
        "question": "What type of dog are you interested in?",
        "options": ["small", "medium", "large"],
        "priority": 1,
        "reasoning": "Size affects behavior patterns"
    }
]"""
        
        try:
            response = self.client.chat(
                model=self.model_name,
                messages=[{"role": "user", "content": test_prompt}]
            )
            
            raw_response = response['message']['content']
            print(f"Debug - Raw response: {raw_response}")
            
            parsed = self._parse_response(raw_response)
            print(f"Debug - Parsed response: {parsed}")
            
            data = json.loads(parsed)
            print(f"Debug - JSON parsed successfully with {len(data)} items")
            return True
            
        except Exception as e:
            print(f"Debug - Error: {e}")
            return False
    
    def _extract_json_from_response(self, response_text: str) -> str:
        """Extract JSON array from LLM response that might be wrapped in text."""
        try:
            response_text = response_text.strip()
            
            # Remove reasoning process if present
            if '<think>' in response_text and '</think>' in response_text:
                start_idx = response_text.find('<think>')
                end_idx = response_text.find('</think>') + len('</think>')
                response_text = response_text[:start_idx] + response_text[end_idx:]
                response_text = response_text.strip()
            
            # Extract JSON from code blocks
            if '```json' in response_text:
                start_idx = response_text.find('```json') + 7
                end_idx = response_text.find('```', start_idx)
                if end_idx != -1:
                    return response_text[start_idx:end_idx].strip()
            elif '```' in response_text:
                start_idx = response_text.find('```') + 3
                end_idx = response_text.find('```', start_idx)
                if end_idx != -1:
                    return response_text[start_idx:end_idx].strip()
            
            # If no code blocks, extract JSON array
            if '[' in response_text:
                start_idx = response_text.find('[')
                bracket_count = 0
                end_idx = -1
                for i in range(start_idx, len(response_text)):
                    if response_text[i] == '[':
                        bracket_count += 1
                    elif response_text[i] == ']':
                        bracket_count -= 1
                        if bracket_count == 0:
                            end_idx = i + 1
                            break
                
                if end_idx != -1:
                    return response_text[start_idx:end_idx].strip()
            
            return response_text.strip()
            
        except Exception as e:
            logger.error(f"Error extracting JSON: {e}")
            return "[]"